<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>On Deep Metric Learning | Kshitij Agrawal - AI Scientist</title><meta name=keywords content="metric-learning"><meta name=description content="We are witnessing a revival of interest in the space of metric learning or representation learning, due to the popularity of embedding models in the NLP space. I thought of penning down some thoughts on the evolution of metric learning over the years, particularly from the point of view of computer vision.
Learning a powerful and generalizable representation has been of particular interest to researchers, as it has applications across a multitude of tasks like classification, person re-identification, face recognition, information retreival and recommender systems."><meta name=author content="Kshitij Agrawal"><link rel=canonical href=https://kshitijagrwl.com/posts/on-metric-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://kshitijagrwl.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://kshitijagrwl.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://kshitijagrwl.com/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://kshitijagrwl.com/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://kshitijagrwl.com/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="On Deep Metric Learning"><meta property="og:description" content="We are witnessing a revival of interest in the space of metric learning or representation learning, due to the popularity of embedding models in the NLP space. I thought of penning down some thoughts on the evolution of metric learning over the years, particularly from the point of view of computer vision.
Learning a powerful and generalizable representation has been of particular interest to researchers, as it has applications across a multitude of tasks like classification, person re-identification, face recognition, information retreival and recommender systems."><meta property="og:type" content="article"><meta property="og:url" content="https://kshitijagrwl.com/posts/on-metric-learning/"><meta property="og:image" content="https://kshitijagrwl.com/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-23T01:50:44+05:30"><meta property="article:modified_time" content="2023-07-23T01:50:44+05:30"><meta property="og:site_name" content="Kshitij Agrawal"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://kshitijagrwl.com/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="On Deep Metric Learning"><meta name=twitter:description content="We are witnessing a revival of interest in the space of metric learning or representation learning, due to the popularity of embedding models in the NLP space. I thought of penning down some thoughts on the evolution of metric learning over the years, particularly from the point of view of computer vision.
Learning a powerful and generalizable representation has been of particular interest to researchers, as it has applications across a multitude of tasks like classification, person re-identification, face recognition, information retreival and recommender systems."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://kshitijagrwl.com/posts/"},{"@type":"ListItem","position":2,"name":"On Deep Metric Learning","item":"https://kshitijagrwl.com/posts/on-metric-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"On Deep Metric Learning","name":"On Deep Metric Learning","description":"We are witnessing a revival of interest in the space of metric learning or representation learning, due to the popularity of embedding models in the NLP space. I thought of penning down some thoughts on the evolution of metric learning over the years, particularly from the point of view of computer vision.\nLearning a powerful and generalizable representation has been of particular interest to researchers, as it has applications across a multitude of tasks like classification, person re-identification, face recognition, information retreival and recommender systems.","keywords":["metric-learning"],"articleBody":"We are witnessing a revival of interest in the space of metric learning or representation learning, due to the popularity of embedding models in the NLP space. I thought of penning down some thoughts on the evolution of metric learning over the years, particularly from the point of view of computer vision.\nLearning a powerful and generalizable representation has been of particular interest to researchers, as it has applications across a multitude of tasks like classification, person re-identification, face recognition, information retreival and recommender systems. It may be based on supervised, weakly supervised or unsupervised learning.\nClassical machine learning approaches (nearest neighbours, SVM) relied on learning a linear distance metric based on a priori knowledge of data. With the advent of deep learning, we are able to learn non-linear feature mapping to learn scalable representations directly from images.\nThe core principle of deep metric learning is to learn a feature representation that brings similar entities closer and dissimilar entities further away, in the latent feature space.\nAdvantages of Deep Metric Learning A specific advantage of learning representations in this way, is that it helps the model handle unseen data. In the real world, a model may be exposed to data it was not trained on and it needs to perform reliably in such setting. This setting is often referred to as open-set problem.\nAdditionally, a compact representation in feature space will enable us to perform k-NN classification, clustering and information retreival with low dimension vectors.\nContrastive Methods 1. Constrastive Learning Chopra et al. 2005 came up with the first contrastive learning approach towards deep metric learning. Their approach had two primary contributions:\nthe metric was computed in the feature space rather than the input space the model was able to handle unseen data at test time Contrastive loss takes a pair of inputs and minimizes the embedding distance when they are from the same class but maximizes the distance otherwise.\n2. Triplet Loss Emerging from research in face recognition, Triplet loss was first proposed in FaceNet (Schroff et al. 2015). The primary motivation of the researchers was to learn a Euclidean mapping for input data where distances directly correspond to measure of face similarity.\nThe formulation selects one anchor sample from a class, one positive sample that is of the same class and one negative sample that belongs to a different class. Triplet loss minimizes the distance between the anchor and positive and maximize the distance between the anchor and negative at the same time with the following equation: where the margin parameter is configured as the minimum offset between distances of similar vs dissimilar pairs.\nKey Considerations Hard negative mining - In every batch, the model needs to select hard samples for the negative to truly learn in every iteration.\nLarge batch size - These approaches rely on a relatively large batch size to compute loss.\nEvaluation Musgrave et al. published a technical report establising a benchmark and comparing the various metric learning approaches. The report claims that for majority of applications, triplet loss and contrastive margin loss perform superior to most mordern techniques.\nThey have an interesting implementation pytorch-metric-learning which comes plugged-in with various loss functions and mining methods.\n","wordCount":"531","inLanguage":"en","datePublished":"2023-07-23T01:50:44+05:30","dateModified":"2023-07-23T01:50:44+05:30","author":{"@type":"Person","name":"Kshitij Agrawal"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://kshitijagrwl.com/posts/on-metric-learning/"},"publisher":{"@type":"Organization","name":"Kshitij Agrawal - AI Scientist","logo":{"@type":"ImageObject","url":"https://kshitijagrwl.com/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://kshitijagrwl.com/ accesskey=h title="Home (Alt + H)"><img src=https://kshitijagrwl.com/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://kshitijagrwl.com/categories/ title=categories><span>categories</span></a></li><li><a href=https://kshitijagrwl.com/tags/ title=tags><span>tags</span></a></li><li><a href=https://kshitijagrwl.com/about/ title=about><span>about</span></a></li><li><a href=https://kshitijagrwl.com/consulting/ title=consulting><span>consulting</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>On Deep Metric Learning</h1><div class=post-meta><span title='2023-07-23 01:50:44 +0530 IST'>23 July 2023</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;531 words&nbsp;·&nbsp;Kshitij Agrawal</div></header><div class=post-content><p>We are witnessing a revival of interest in the space of metric learning or representation learning, due to the popularity of embedding models in the NLP space. I thought of penning down some thoughts on the evolution of metric learning over the years, particularly from the point of view of computer vision.</p><p>Learning a powerful and generalizable representation has been of particular interest to researchers, as it has applications across a multitude of tasks like classification, person re-identification, face recognition, information retreival and recommender systems. It may be based on supervised, weakly supervised or unsupervised learning.</p><p>Classical machine learning approaches (nearest neighbours, SVM) relied on learning a linear distance metric based on a priori knowledge of data. With the advent of deep learning, we are able to learn non-linear feature mapping to learn scalable representations directly from images.</p><p>The core principle of deep metric learning is to learn a feature representation that brings similar entities closer and dissimilar entities further away, in the latent feature space.</p><p><img loading=lazy src=/metric-learning-01.png alt></p><h2 id=advantages-of-deep-metric-learning>Advantages of Deep Metric Learning<a hidden class=anchor aria-hidden=true href=#advantages-of-deep-metric-learning>#</a></h2><ol><li><p>A specific advantage of learning representations in this way, is that it helps the model handle unseen data. In the real world, a model may be exposed to data it was not trained on and it needs to perform reliably in such setting. This setting is often referred to as <em>open-set</em> problem.</p></li><li><p>Additionally, a compact representation in feature space will enable us to perform k-NN classification, clustering and information retreival with low dimension vectors.</p></li></ol><h1 id=contrastive-methods>Contrastive Methods<a hidden class=anchor aria-hidden=true href=#contrastive-methods>#</a></h1><h3 id=1-constrastive-learning>1. Constrastive Learning<a hidden class=anchor aria-hidden=true href=#1-constrastive-learning>#</a></h3><p>Chopra et al. 2005 came up with the first contrastive learning approach towards deep metric learning. Their approach had two primary contributions:</p><ul><li>the metric was computed in the feature space rather than the input space</li><li>the model was able to handle unseen data at test time</li></ul><p>Contrastive loss takes a pair of inputs and minimizes the embedding distance when they are from the same class but maximizes the distance otherwise.</p><h3 id=2-triplet-loss>2. Triplet Loss<a hidden class=anchor aria-hidden=true href=#2-triplet-loss>#</a></h3><p>Emerging from research in face recognition, Triplet loss was first proposed in FaceNet (Schroff et al. 2015). The primary motivation of the researchers was to learn a Euclidean mapping for input data where distances directly correspond to measure of face similarity.</p><p><img loading=lazy src=/metric-learning-02.png alt></p><p>The formulation selects one <em>anchor</em> sample from a class, one <em>positive</em> sample that is of the same class and one <em>negative</em> sample that belongs to a different class. Triplet loss minimizes the distance between the anchor and positive and maximize the distance between the anchor and negative at the same time with the following equation:
where the margin parameter is configured as the minimum offset between distances of similar vs dissimilar pairs.</p><h2 id=key-considerations>Key Considerations<a hidden class=anchor aria-hidden=true href=#key-considerations>#</a></h2><ol><li><p>Hard negative mining - In every batch, the model needs to select hard samples for the negative to truly learn in every iteration.</p></li><li><p>Large batch size - These approaches rely on a relatively large batch size to compute loss.</p></li></ol><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><p>Musgrave et al. published a <a href=https://arxiv.org/pdf/2003.08505.pdf>technical report</a> establising a benchmark and comparing the various metric learning approaches. The report claims that for majority of applications, triplet loss and contrastive margin loss perform superior to most mordern techniques.</p><p>They have an interesting implementation <a href=https://github.com/KevinMusgrave/pytorch-metric-learning>pytorch-metric-learning</a> which comes plugged-in with various loss functions and mining methods.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://kshitijagrwl.com/tags/metric-learning/>metric-learning</a></li></ul><nav class=paginav><a class=next href=https://kshitijagrwl.com/posts/first-post/><span class=title>Next »</span><br><span>Welcome to my tech blog</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://kshitijagrwl.com/>Kshitij Agrawal - AI Scientist</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>